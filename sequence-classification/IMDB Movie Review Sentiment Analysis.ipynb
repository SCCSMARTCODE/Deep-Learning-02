{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport gc\nimport pandas as pd\nimport nltk\nimport torch\nimport torch.nn as nn\nimport zipfile\nimport re\nnltk.data.path.append('/usr/local/share/nltk_data') \nnltk.download('wordnet', download_dir='/usr/local/share/nltk_data') \nnltk.download('stopwords')  \nnltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom bs4 import BeautifulSoup\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import TensorDataset, DataLoader, random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T05:03:14.067425Z","iopub.execute_input":"2024-10-16T05:03:14.068168Z","iopub.status.idle":"2024-10-16T05:03:20.978345Z","shell.execute_reply.started":"2024-10-16T05:03:14.068128Z","shell.execute_reply":"2024-10-16T05:03:20.977447Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /usr/local/share/nltk_data...\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')  \n\ntexts = \" \".join(df['review'].tolist()).lower()  \n\ntexts = BeautifulSoup(texts, 'html.parser').get_text()  \ncleaned_texts = re.sub(r'[^a-zA-Z\\s]', ' ', texts).lower()\n\nwords = word_tokenize(cleaned_texts)\n\n\nwith zipfile.ZipFile('/usr/local/share/nltk_data/corpora/wordnet.zip', 'r') as zip_ref:\n    zip_ref.extractall('/usr/local/share/nltk_data/corpora/')\n    \nlemmatizer = WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))  \nfiltered_lemmatized_words = [  \n    lemmatizer.lemmatize(word) for word in words if word not in stop_words  \n] \n\nvocab = sorted(set(filtered_lemmatized_words))  # Unique words only  \nvocab.insert(0, '<PAD>')  # Add padding token  \n\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}  \nidx_to_word = {idx: word for idx, word in enumerate(vocab)}  \n\nprint(\"Vocabulary Size:\", len(vocab))  \nprint(\"Sample Vocabulary:\", vocab[1310:1320])","metadata":{"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Vocabulary Size: 91725\nSample Vocabulary: ['agar', 'agashe', 'agate', 'agatha', 'agathaclosing', 'agbayani', 'age', 'aged', 'agee', 'ageing']\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = []\nlabels = []\n\nfor review, sentiment in zip(df['review'], df['sentiment']):\n    review = review.lower()\n\n    review = BeautifulSoup(review, 'html.parser').get_text()\n    review = re.sub(r'[^a-zA-Z\\s]', ' ', review)\n\n    tokens = word_tokenize(review)\n    filtered_lemmatized_tokens = [\n        lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n    ]\n    \n    input_indices = [word_to_idx[word] for word in filtered_lemmatized_tokens if word in word_to_idx]\n    \n    label = 1 if sentiment == 'positive' else 0\n\n    inputs.append(input_indices)\n    labels.append(label)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:05:48.150036Z","iopub.execute_input":"2024-10-16T05:05:48.150358Z","iopub.status.idle":"2024-10-16T05:08:15.688801Z","shell.execute_reply.started":"2024-10-16T05:05:48.150326Z","shell.execute_reply":"2024-10-16T05:08:15.687883Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/874138525.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  review = BeautifulSoup(review, 'html.parser').get_text()\n","output_type":"stream"}]},{"cell_type":"code","source":"max_len = 256\ninput_tensors = [torch.tensor(input) for input in inputs]\n    \n\n# Pad the sequences\npadded_inputs = pad_sequence([seq[:max_len] for seq in input_tensors], batch_first=True, padding_value=word_to_idx['<PAD>'])\n\ndataset = TensorDataset(padded_inputs, torch.tensor(labels, dtype=torch.float))\ntrain_ds, valid_ds, test_ds = random_split(dataset, [35000, 7500, 7500])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:08:15.689992Z","iopub.execute_input":"2024-10-16T05:08:15.690311Z","iopub.status.idle":"2024-10-16T05:08:18.512477Z","shell.execute_reply.started":"2024-10-16T05:08:15.690279Z","shell.execute_reply":"2024-10-16T05:08:18.511656Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"hyperparameters = {\n    \"batch_size\": 256,\n    \"embedding_dim\": 128,\n    \"num_embeddings\": len(idx_to_word),\n    \"epochs\": 30,\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:03.224242Z","iopub.execute_input":"2024-10-16T05:10:03.224700Z","iopub.status.idle":"2024-10-16T05:10:03.230010Z","shell.execute_reply.started":"2024-10-16T05:10:03.224661Z","shell.execute_reply":"2024-10-16T05:10:03.228923Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=hyperparameters['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nvalid_dl = DataLoader(valid_ds, batch_size=hyperparameters['batch_size'], drop_last=True, num_workers=2, pin_memory=True)\ntest_dl = valid_dl = DataLoader(test_ds, batch_size=hyperparameters['batch_size'], drop_last=True, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:07.016630Z","iopub.execute_input":"2024-10-16T05:10:07.017555Z","iopub.status.idle":"2024-10-16T05:10:07.024571Z","shell.execute_reply.started":"2024-10-16T05:10:07.017509Z","shell.execute_reply":"2024-10-16T05:10:07.023252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CustomDL:\n    def __init__(self, dataloader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.dataloader = dataloader\n        self.device = device\n\n    def __iter__(self):\n        for inputs, targets in self.dataloader:\n            inputs = inputs.to(self.device)\n            targets = targets.to(self.device)\n\n            yield inputs, targets\n\n            del inputs, targets\n            torch.cuda.empty_cache() if self.device == 'cuda' else None\n            gc.collect()\n\n    def __len__(self):\n        return len(self.dataloader)\n\ncustom_train_dl = CustomDL(train_dl)\ncustom_valid_dl = CustomDL(valid_dl)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:08.721082Z","iopub.execute_input":"2024-10-16T05:10:08.722089Z","iopub.status.idle":"2024-10-16T05:10:08.795326Z","shell.execute_reply.started":"2024-10-16T05:10:08.722035Z","shell.execute_reply":"2024-10-16T05:10:08.794145Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class SANetwork(nn.Module):\n        \n    def __init__(self, embedding_dim, num_embeddings):\n        super(SANetwork, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n        self.lstm_cells = nn.LSTM(input_size=embedding_dim, hidden_size=256, num_layers=2, batch_first=True, dropout=.4)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=256, out_features=1),\n        )\n        \n        \n\n    def forward(self, input, hidden=None):\n        x = self.embedding(input)\n        x, hidden = self.lstm_cells(x, hidden)\n        x = x[:, -1, :]\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:22.754532Z","iopub.execute_input":"2024-10-16T05:10:22.755639Z","iopub.status.idle":"2024-10-16T05:10:22.765570Z","shell.execute_reply.started":"2024-10-16T05:10:22.755591Z","shell.execute_reply":"2024-10-16T05:10:22.764443Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"network = SANetwork(hyperparameters['embedding_dim'], hyperparameters['num_embeddings'])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnetwork = network.to(device)\n# summary(network, input_size=(1,40))","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:26.032685Z","iopub.execute_input":"2024-10-16T05:10:26.033330Z","iopub.status.idle":"2024-10-16T05:10:26.436546Z","shell.execute_reply.started":"2024-10-16T05:10:26.033290Z","shell.execute_reply":"2024-10-16T05:10:26.435326Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(len(idx_to_word))","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:10:28.752653Z","iopub.execute_input":"2024-10-16T05:10:28.753517Z","iopub.status.idle":"2024-10-16T05:10:28.758353Z","shell.execute_reply.started":"2024-10-16T05:10:28.753476Z","shell.execute_reply":"2024-10-16T05:10:28.757220Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"91725\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = torch.nn.BCEWithLogitsLoss()\noptimizer = Adam(network.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:11:02.836155Z","iopub.execute_input":"2024-10-16T05:11:02.836598Z","iopub.status.idle":"2024-10-16T05:11:04.275917Z","shell.execute_reply.started":"2024-10-16T05:11:02.836553Z","shell.execute_reply":"2024-10-16T05:11:04.275105Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train(network, train_loader, val_loader, optimizer, criterion, scheduler, device, num_epochs):\n    for epoch in range(num_epochs):\n        network.train()\n\n        total_loss = 0.0\n        for batch in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n            inputs, targets = batch\n\n            optimizer.zero_grad()\n\n            outputs = network(inputs)\n\n            loss = criterion(outputs.view(-1), targets.view(-1))\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Training loss for epoch {epoch + 1}: {avg_loss:.4f}\")\n\n        network.eval()\n        val_loss = 0.0\n        correct_predictions = 0\n        total_predictions = 0\n\n        with torch.no_grad():\n            for val_batch in val_loader:\n                val_inputs, val_targets = val_batch\n                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n\n                val_outputs = network(val_inputs)\n\n                v_loss = criterion(val_outputs.view(-1), val_targets.view(-1))\n                val_loss += v_loss.item()\n\n                predicted = (val_outputs >= 0.5).float()\n                correct_predictions += (predicted.view(-1) == val_targets.view(-1)).sum().item()\n                total_predictions += val_targets.size(0)\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_accuracy = correct_predictions / total_predictions\n        print(f\"Validation loss for epoch {epoch + 1}: {avg_val_loss:.4f}, Accuracy: {val_accuracy * 100:.2f}%\")\n\n        if scheduler is not None:\n            scheduler.step()\n\n        torch.save(network.state_dict(), \"parameters.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:13:04.499206Z","iopub.execute_input":"2024-10-16T06:13:04.499649Z","iopub.status.idle":"2024-10-16T06:13:04.512155Z","shell.execute_reply.started":"2024-10-16T06:13:04.499592Z","shell.execute_reply":"2024-10-16T06:13:04.511234Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train(network, custom_train_dl, custom_valid_dl, optimizer, criterion, None, device, hyperparameters['epochs'])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:58:03.672542Z","iopub.execute_input":"2024-10-16T05:58:03.673166Z","iopub.status.idle":"2024-10-16T05:58:03.677710Z","shell.execute_reply.started":"2024-10-16T05:58:03.673119Z","shell.execute_reply":"2024-10-16T05:58:03.676619Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def inference(network, test_loader, criterion, device):\n    network.eval()\n    test_loss = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad(): \n        for batch in test_loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            outputs = network(inputs)\n            outputs = torch.sigmoid(outputs) \n\n            loss = criterion(outputs.view(-1), targets.view(-1))\n            test_loss += loss.item()\n\n            predicted = (outputs >= 0.5).float()\n\n            correct_predictions += (predicted.view(-1) == targets.view(-1)).sum().item()\n            total_predictions += targets.size(0)\n\n    avg_test_loss = test_loss / len(test_loader)\n    test_accuracy = correct_predictions / total_predictions\n\n    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n    return avg_test_loss, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-16T05:58:07.129390Z","iopub.execute_input":"2024-10-16T05:58:07.129789Z","iopub.status.idle":"2024-10-16T05:58:07.139439Z","shell.execute_reply.started":"2024-10-16T05:58:07.129750Z","shell.execute_reply":"2024-10-16T05:58:07.138636Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"inference(network, test_dl, criterion, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:13:18.311472Z","iopub.execute_input":"2024-10-16T06:13:18.311855Z","iopub.status.idle":"2024-10-16T06:13:19.714751Z","shell.execute_reply.started":"2024-10-16T06:13:18.311818Z","shell.execute_reply":"2024-10-16T06:13:19.713731Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Test Loss: 0.5286, Test Accuracy: 97.00%\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(0.5285909998005834, 0.9699622844827587)"},"metadata":{}}]},{"cell_type":"code","source":"def exterior_inference(str_=\"\"):  \n    network.eval()  \n    with torch.no_grad():  \n        # Preprocess the input string  \n        str_ = str_.lower()  \n        str_ = BeautifulSoup(str_, 'html.parser').get_text()  \n        str_ = re.sub(r'[^a-zA-Z\\s]', ' ', str_)  \n\n        words = word_tokenize(str_)  \n        lemmatizer = WordNetLemmatizer()  \n        stop_words = set(stopwords.words('english'))  \n        processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  \n\n        # Convert words to indices  \n        input_ = [word_to_idx[word] for word in processed_words if word in word_to_idx]  \n        input_ = torch.tensor(input_).to(device).unsqueeze(dim=0)  \n\n        # Get the model's output  \n        result = nn.Sigmoid()(network(input_))  \n\n        # Determine sentiment  \n        sentiment_score = result.item()  # Get the score as a float  \n        if sentiment_score >= 0.7:  \n            sentiment = \"Positive\"  \n            description = \"This text conveys positive emotions.\"  \n        elif sentiment_score <= 0.3:  \n            sentiment = \"Negative\"  \n            description = \"This text conveys negative emotions.\"  \n        else:  \n            sentiment = \"Neutral\"  \n            description = \"This text is neutral in sentiment.\"  \n\n        # Print the output  \n        print(\"-\" * 50)  \n        print(f\"Input Text: {str_}\")  \n        print(f\"Sentiment Score: {sentiment_score:.4f} ({sentiment})\")  \n        print(f\"Description: {description}\")  \n        print(\"-\" * 50)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:30:13.682489Z","iopub.execute_input":"2024-10-16T06:30:13.683300Z","iopub.status.idle":"2024-10-16T06:30:13.693591Z","shell.execute_reply.started":"2024-10-16T06:30:13.683257Z","shell.execute_reply":"2024-10-16T06:30:13.692582Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"exterior_inference(\"I had a horrible experience at the hotel. The room was dirty, and the staff were rude\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:32:46.842832Z","iopub.execute_input":"2024-10-16T06:32:46.843613Z","iopub.status.idle":"2024-10-16T06:32:46.851495Z","shell.execute_reply.started":"2024-10-16T06:32:46.843570Z","shell.execute_reply":"2024-10-16T06:32:46.850665Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"--------------------------------------------------\nInput Text: i had a horrible experience at the hotel  the room was dirty  and the staff were rude\nSentiment Score: 0.0532 (Negative)\nDescription: This text conveys negative emotions.\n--------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"exterior_inference(\"I absolutely love this product! It has exceeded all my expectations\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:34:20.509406Z","iopub.execute_input":"2024-10-16T06:34:20.510326Z","iopub.status.idle":"2024-10-16T06:34:20.518730Z","shell.execute_reply.started":"2024-10-16T06:34:20.510281Z","shell.execute_reply":"2024-10-16T06:34:20.517712Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"--------------------------------------------------\nInput Text: i absolutely love this product  it has exceeded all my expectations\nSentiment Score: 0.9837 (Positive)\nDescription: This text conveys positive emotions.\n--------------------------------------------------\n","output_type":"stream"}]}]}