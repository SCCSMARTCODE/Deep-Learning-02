# Text Generation using LSTM (Shakespeare)

## Overview
This project demonstrates character-level text generation using a Long Short-Term Memory (LSTM) network. The model is trained on Shakespeare's works and generates new text character by character, learning the patterns and style of Shakespearean language.

## Dataset
The dataset used for training is a collection of works by William Shakespeare. It is treated as a single sequence of characters, with the model trained to predict the next character given a sequence of preceding characters.

## Model
The model is built using an LSTM architecture. Key components:
- **Embedding Layer**: Converts character indices into dense vectors.
- **LSTM Layer**: Captures temporal dependencies in the text sequence.
- **Fully Connected Layer**: Maps LSTM outputs to the character space for prediction.

## How it Works
1. The model is trained to predict the next character in a sequence given the preceding characters.
2. **Temperature sampling** is used to adjust the creativity and randomness of the generated text.
3. The model generates new text by feeding its own predictions back into the input, creating a loop of text generation.

## Usage

### Clone the Repository:
```bash
git clone https://github.com/sccsmartcode/Deep-Learning-02/text-generation.git
cd text-generation
```

## Example Generated Text:
Hereâ€™s an example of text generated by the model after training:
```
Once upon a time there was a man,
Whose love was like the summer's day,
He spoke with grace and light.
```

## Future Improvements
- Use more advanced architectures like GRU or Transformer.
- Train on a larger corpus of text to improve generalization.

## License
This project is licensed under the MIT License.

---
