{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SCCSMARTCODE/Deep-Learning-02/blob/main/text-generation/Word_Level_Text_Generation_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preparation\n",
        "- [x] Download and load the Shakespeare text or any text file.\n",
        "- [x] Convert the text to lowercase and split it into words (tokenization).\n",
        "- [x] Create a vocabulary of unique words and create word-to-index and index-to-word mappings.\n",
        "\n",
        "#### Data Preprocessing\n",
        "- [x] Set a sequence length (e.g., 10 words per sequence).\n",
        "- [x] Create input sequences and corresponding target words (the next word following each sequence).\n",
        "- [x] Convert the sequences and targets to tensors (PyTorch format).\n",
        "\n",
        "#### Build the LSTM Model\n",
        "- [x] Define a PyTorch LSTM model with:\n",
        "  - An embedding layer to map word indices to dense vectors.\n",
        "  - LSTM layers to capture sequential word patterns.\n",
        "  - A fully connected output layer to predict the next word.\n",
        "- [x] Set model parameters (e.g., vocab size, embedding size, hidden units).\n",
        "\n",
        "#### Training the Model\n",
        "- [x] Define the loss function and the optimizer.\n",
        "- [x] Implement a training loop to:\n",
        "  - Split data into batches.\n",
        "  - Feed batches into the model.\n",
        "  - Compute loss and update the model parameters.\n",
        "  - Track and print training progress.\n",
        "\n",
        "#### Text Generation\n",
        "- [x] Create a text generation function:\n",
        "  - Feed a seed text sequence to the trained model.\n",
        "  - Predict the next word, append it to the seed, and repeat.\n",
        "  - Generate a specified number of words based on the initial seed.\n",
        "\n",
        "#### Evaluation and Optimization\n",
        "- [x] Evaluate the quality of generated text for coherence and structure.\n",
        "- [x] Adjust hyperparameters (embedding size, hidden units, layers) for better results.\n",
        "- [x] Experiment with different sequence lengths and potentially use temperature to control the randomness of predictions."
      ],
      "metadata": {
        "id": "LNX_fd0ZD3RG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEfhDF5q5YIc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare_short.txt\", 'r') as f:\n",
        "    texts = f.read()"
      ],
      "metadata": {
        "id": "h6LWtHlcEAlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01iu5qMZW1eA",
        "outputId": "1470c424-add5-4b40-c447-fc03ea55c84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words = re.findall(r'\\b\\w+\\b', texts.lower())\n",
        "idx_to_word = {idx: word for idx, word in enumerate(list(sorted(set(tokenized_words))))}\n",
        "word_to_idx = {word: idx for idx, word in enumerate(list(sorted(set(tokenized_words))))}"
      ],
      "metadata": {
        "id": "mDb-dKowFwOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(seq_length=10, text=tokenized_words):\n",
        "    input = []\n",
        "    label = []\n",
        "\n",
        "    for x in range(len(text)-seq_length):\n",
        "        input.append([word_to_idx[word] for word in text[x:x+seq_length]])\n",
        "        label.append([word_to_idx[text[x+seq_length]]])\n",
        "    return input, label"
      ],
      "metadata": {
        "id": "JKtcaDopH-5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = create_dataset(seq_length=40, text=tokenized_words)\n",
        "inputs = dataset[0]\n",
        "labels = dataset[1]\n",
        "\n",
        "for x in range(len(labels)-1):\n",
        "    print([idx_to_word[idx] for idx in inputs[x]], idx_to_word[labels[x][0]])\n",
        "    break"
      ],
      "metadata": {
        "id": "5AC1m5yLJrFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ff27bc-bb9e-42d6-b0e3-6fe8f07376a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', 'all', 'resolved', 'resolved', 'first', 'citizen', 'first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the'] people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'num_embeddings': len(word_to_idx),\n",
        "    'embedding_dim': 100,\n",
        "    'batch_size': 2048,\n",
        "    'num_epochs': 10,\n",
        "    'learning_rate': 0.01,\n",
        "    'max_lr': 0.01,\n",
        "    'param_save_path': \"parameter.pth\"\n",
        "}"
      ],
      "metadata": {
        "id": "icfT6-O2g0N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLevelTextGenNetwork(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim):\n",
        "        super(WordLevelTextGenNetwork, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=512,\n",
        "                            num_layers=3, batch_first=True\n",
        "                           )\n",
        "\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(in_features=1024, out_features=num_embeddings)\n",
        "        )\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        x = self.embedding(input)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc_layers(x)\n",
        "\n",
        "        return x, hidden\n",
        "network = WordLevelTextGenNetwork(hyperparameters['num_embeddings'], hyperparameters['embedding_dim'])\n"
      ],
      "metadata": {
        "id": "By6ySt-MNqQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "network = network.to(device)\n",
        "network.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep Learning/word_text_gen_parameters_2nd_post.pth\"))\n"
      ],
      "metadata": {
        "id": "Yqr9sStjoTvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c966b1c8-56de-43ed-bca8-fb150429f32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1b2764238264>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  network.load_state_dict(torch.load(\"/content/drive/MyDrive/Deep Learning/word_text_gen_parameters_2nd.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Generated data point: [==={len(labels)}===]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BidclfGZVwO2",
        "outputId": "af1dd396-edd1-4493-a97c-1723ba1b85d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Generated data point: [===208490===]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(torch.tensor(inputs), torch.tensor(labels))\n",
        "dataloader = DataLoader(dataset, batch_size=hyperparameters['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "# scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "#                                      max_lr=hyperparameters['max_lr'],\n",
        "#                                      total_steps=hyperparameters['num_epochs'] * (len(dataloader)),\n",
        "#                                     )"
      ],
      "metadata": {
        "id": "X4-ajO3cW-q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, data_loader, optimizer, criterion, scheduler, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        network.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for batch in data_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.view(-1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, _ = network(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Training loss for epoch {epoch + 1}: {avg_loss:.4f}\")\n",
        "\n",
        "        torch.save(network.state_dict(), \"/content/drive/MyDrive/Deep Learning/word_text_gen_parameters_2nd_post.pth\")\n"
      ],
      "metadata": {
        "id": "xEGeS0irgeP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(network, dataloader, optimizer, criterion, None, device, hyperparameters['num_epochs'])"
      ],
      "metadata": {
        "id": "SsGG0QJGu9hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128619d7-42a3-4a48-f94b-16f29766ddf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss for epoch 1: 1.0516\n",
            "Training loss for epoch 2: 0.8193\n",
            "Training loss for epoch 3: 0.7505\n",
            "Training loss for epoch 4: 0.7153\n",
            "Training loss for epoch 5: 0.6714\n",
            "Training loss for epoch 6: 0.6216\n",
            "Training loss for epoch 7: 0.5748\n",
            "Training loss for epoch 8: 0.5311\n",
            "Training loss for epoch 9: 0.4943\n",
            "Training loss for epoch 10: 0.4662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_text(model, start_str, n_words, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    input_words = start_str.split()\n",
        "    input_indices = [word_to_idx[word] for word in input_words if word in word_to_idx.keys()]\n",
        "\n",
        "    input_seq = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_text = start_str\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        output = output / temperature\n",
        "\n",
        "        if output.dim() == 2:\n",
        "            probs = torch.softmax(output, dim=1).detach().cpu().numpy()\n",
        "        else:\n",
        "            probs = torch.softmax(output[:, -1], dim=1).detach().cpu().numpy()\n",
        "\n",
        "        if probs.size == 0:\n",
        "            break\n",
        "\n",
        "        next_word_idx = np.random.choice(len(word_to_idx), p=probs[0])\n",
        "        if next_word_idx not in idx_to_word:\n",
        "            break\n",
        "\n",
        "        next_word = idx_to_word[next_word_idx]\n",
        "        generated_text += \" \" + next_word\n",
        "\n",
        "        input_seq = torch.tensor([[next_word_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "U-62XDh7hTTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_str = \"What do you have to say\"\n",
        "generated_text = generate_text(network, start_str, 100, temperature=0.8)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1lyFfD2uHzO",
        "outputId": "d2c12e95-a7c8-43d6-930e-35631d9f2d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you have to say she hath given no wit to put by the world second servant serves your sister is made so past though yea and i prithee wrong these fair men if love say your trial to be thought to part a man or liberty i conjure him that stands his wife leontes next she will call her here duke vincentio one of orders natural than this i hate to effect camillo there s no love a gentleman gainst so your fingers growing nor he seem beyond the child of a wonder for surely sir the duke hath been to my good son\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(network.state_dict(), 'para.pth')"
      ],
      "metadata": {
        "id": "QjMgfK-CQrHT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}